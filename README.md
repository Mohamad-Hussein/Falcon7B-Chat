# Falcon7B-Chat
This is an attempt to configure a chatbot using the falcon-7b-instruct parameter model to run locally on a machine with &lt;8Gb VRAM. Using 4 bit quantization to reduce memory load. Source to [guide](https://www.mlexpert.io/prompt-engineering/chatbot-with-local-llm-using-langchain).
